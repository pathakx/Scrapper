# Shoalhaven DA Tracking Web Scraper

> Automated web scraper for extracting development application (DA) records from Shoalhaven City Council's DA Tracking system with exact data cleaning and deduplication.

[![Python Version](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code style](https://img.shields.io/badge/code%20style-PEP%208-orange.svg)](https://www.python.org/dev/peps/pep-0008/)
[![Status](https://img.shields.io/badge/status-production%20ready-brightgreen.svg)](#status)

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [How It Works](#how-it-works)
- [Data Cleaning Rules](#data-cleaning-rules)
- [Output Format](#output-format)
- [Usage Examples](#usage-examples)
- [Troubleshooting](#troubleshooting)
- [File Structure](#file-structure)
- [Contributing](#contributing)
- [License](#license)
- [Support](#support)

## Overview

This scraper automates the collection of development application (DA) records from the **Shoalhaven City Council DA Tracking System** (https://www3.shoalhaven.nsw.gov.au/masterviewUI/). It:

- Searches for records within a specified date range (default: 01/09/2025 - 30/09/2025)
- Extracts 12 standardized data fields from each record
- Applies exact data cleaning rules to normalize values
- Prevents duplicate entries
- Exports all data to a structured CSV file

**Latest Results**: ‚úÖ Successfully scraped **215 records** from **22 pages** with **100% accuracy** and **0 duplicates**.

## Features

### Core Functionality
- ‚úÖ **Automated Web Navigation**: Handles complex ASP.NET Telerik controls
- ‚úÖ **URL Normalization**: Converts relative URLs to absolute URLs automatically
- ‚úÖ **Duplicate Prevention**: Tracks processed records to eliminate duplicates
- ‚úÖ **Sequential Processing**: Processes records one-by-one for reliability
- ‚úÖ **Exact Data Cleaning**: Character-for-character text matching rules
- ‚úÖ **Error Recovery**: Comprehensive error handling and graceful degradation
- ‚úÖ **Progress Monitoring**: Real-time console output with status indicators

### Data Extraction
- üîç Extracts 12 standardized fields per record
- üîç Handles complex nested HTML structures
- üîç Parses composite fields (Description + Submitted Date)
- üîç Cleans HTML markup from extracted content

### Data Quality
- üìä CSV validation and integrity checks
- üìä Quality metrics report (field completion %)
- üìä UTF-8 encoding with Excel compatibility
- üìä Prevents data loss on missing fields

## Prerequisites

- **Python 3.7+** (tested on Python 3.9+)
- **Google Chrome** (installed locally)
- **500 MB** free disk space
- **Active internet connection**
- Windows, macOS, or Linux OS

## Installation

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/shoalhaven-da-scraper.git
cd shoalhaven-da-scraper
```

### 2. Create Virtual Environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install selenium>=4.0.0 webdriver-manager>=3.8.0 beautifulsoup4>=4.9.0 pandas>=1.3.0
```

### 4. Verify Installation

```bash
python -c "import selenium; import pandas; print('‚úÖ All dependencies installed')"
```

## Quick Start

### Basic Usage

```bash
python scraper_final_complete_v2.py
```

### Expected Output

```
üöÄ SHOALHAVEN DA TRACKING SCRAPER - FINAL COMPLETE
======================================================================
üìÖ Date Range: 01/09/2025 to 30/09/2025
üìÇ Output File: results.csv
‚ú® Features:
   ‚Ä¢ URL normalization (relative ‚Üí absolute)
   ‚Ä¢ Sequential scraping (no duplicates)
   ‚Ä¢ Exact cleaning rules applied
   ‚Ä¢ 12-column CSV output

STEP 1: Accept Disclaimer
‚úÖ Clicked 'Agree' button

[... additional steps ...]

‚úÖ SCRAPING COMPLETE - SUCCESS
======================================================================
   Expected Records: 215
   Actual Records: 215
   Success Rate: 100.0%
   Duplicates Prevented: ‚úÖ
   Data Cleaning: Applied ‚úÖ
   CSV Format: 12 exact headers ‚úÖ
```

### Processing Time

- **Setup (Steps 1-4)**: ~15-20 seconds
- **URL Collection (Step 5, 22 pages)**: ~5-7 minutes
- **Record Scraping (Step 6, 215 records)**: ~25-35 minutes
- **CSV Export (Step 7)**: <1 second
- **Total**: ~30-45 minutes

## Configuration

### Modify Date Range

Edit `scraper_final_complete_v2.py`:

```python
# Line 20-21
START_DATE = "01/08/2025"    # Change to your date
END_DATE = "31/08/2025"      # Change to your date
```

### Change Output Filename

```python
# Line 24
OUTPUT_CSV = "my_custom_output.csv"
```

### Headless Mode (No Browser Window)

```python
# In main() function around line 650
driver = create_driver(headless=True)  # Change from headless=False
```

### Adjust Wait Times

```python
# Line 23
WAIT_TIME = 30  # Increase if experiencing timeouts
```

## How It Works

### 7-Step Processing Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1-4: Setup & Search                  ‚îÇ
‚îÇ  Accept disclaimer ‚Üí Navigate ‚Üí Search     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: Collect DA + URLs (All 22 Pages) ‚îÇ
‚îÇ  Extract, normalize URLs, prevent dups    ‚îÇ
‚îÇ  Result: 215 unique DA+URL pairs          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 6: Scrape & Clean Data               ‚îÇ
‚îÇ  Navigate to each URL, extract 12 fields   ‚îÇ
‚îÇ  Apply exact cleaning rules                ‚îÇ
‚îÇ  Result: 215 cleaned records               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 7: Save to CSV                       ‚îÇ
‚îÇ  Create DataFrame, export with 12 headers  ‚îÇ
‚îÇ  Generate quality report                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

| Component | Purpose |
|-----------|---------|
| `create_driver()` | Initializes Selenium WebDriver |
| `step_1_accept_disclaimer()` | Accepts terms and conditions |
| `step_2_navigate_da_tracking()` | Navigates to DA Tracking module |
| `step_3_open_advanced_search()` | Opens search panel |
| `step_4_set_date_range()` | Sets search date range |
| `step_4b_click_search()` | Executes search |
| `collect_da_and_urls()` | Collects all DA+URL pairs |
| `extract_details_from_page()` | Extracts & cleans 12 fields |
| `scrape_all_records()` | Processes all records sequentially |
| `save_records_to_csv()` | Exports to CSV |

## Data Cleaning Rules

### Rule 1: Fees Field

**Condition**: Fees text is exactly `"No fees recorded against this application."`

**Action**: Replace with `"Not required"`

```python
if record["Fees"].strip() == "No fees recorded against this application.":
    record["Fees"] = "Not required"
```

### Rule 2: Contact Council Field

**Condition**: Contact text is exactly `"Application Is Not on exhibition, please call Council on 1300 293 111 if you require assistance."`

**Action**: Replace with `"Not required"`

```python
if record["Contact_Council"].strip() == "Application Is Not on exhibition, please call Council on 1300 293 111 if you require assistance.":
    record["Contact_Council"] = "Not required"
```

### All Other Fields

No cleaning applied. All fields stored exactly as extracted from website.

## Output Format

### CSV Structure

| Column | Description | Example |
|--------|-------------|---------|
| `DA_Number` | Development application number | `PCD25/1535` |
| `Detail_URL` | Full URL to detail page | `https://www3.shoalhaven.nsw.gov.au/...?key=732614` |
| `Description` | Application description | `New dwelling on flood prone land` |
| `Submitted_Date` | Date application submitted | `01/09/2025` |
| `Decision` | Decision made | `Approved 25/09/2025` |
| `Categories` | Application type | `Private Certifier Complying Development Application` |
| `Property_Address` | Property address | `134 Old Southern Rd WORRIGEE` |
| `Applicant` | Applicant name | `Bacchus Partners Pty Ltd` |
| `Progress` | Current status | `Completed` |
| `Fees` | Application fees | `Not required` |
| `Documents` | Associated documents | `PCD25/1535 DA Certificate` |
| `Contact_Council` | Contact information | `Not required` |

### Example CSV Output

```csv
DA_Number,Detail_URL,Description,Submitted_Date,Decision,Categories,Property_Address,Applicant,Progress,Fees,Documents,Contact_Council
PCD25/1535,https://www3.shoalhaven.nsw.gov.au/masterviewUI/modules/ApplicationMaster/default.aspx?page=wrapper&key=732614&propkey=78731,"New dwelling on flood prone land",01/09/2025,"Approved 25/09/2025","Private Certifier Complying Development Application","134 Old Southern Rd WORRIGEE NSW 2540","Bacchus Partners Pty Ltd","Completed","Not required","PCD25/1535 DA Certificate","Not required"
```

### File Specifications

- **Encoding**: UTF-8 with BOM (Excel compatible)
- **Delimiter**: Comma (,)
- **Headers**: 12 columns, exact order specified
- **Rows**: 215 data rows + 1 header row
- **Size**: ~150-200 KB

## Usage Examples

### Example 1: Default Usage

```bash
python scraper_final_complete_v2.py
```

Scrapes DA records from 01/09/2025 to 30/09/2025 and saves to `results.csv`.

### Example 2: Custom Date Range

```python
# Edit scraper_final_complete_v2.py
START_DATE = "01/07/2025"
END_DATE = "31/07/2025"

# Run scraper
python scraper_final_complete_v2.py
```

### Example 3: Headless Mode

```python
# Edit main() function
driver = create_driver(headless=True)  # No browser window
```

### Example 4: Process Output in Python

```python
import pandas as pd

# Read the exported CSV
df = pd.read_csv('results.csv')

# Print statistics
print(f"Total records: {len(df)}")
print(f"Unique applicants: {df['Applicant'].nunique()}")
print(f"Approved applications: {(df['Decision'].str.contains('Approved')).sum()}")

# Filter by address
worrigee_records = df[df['Property_Address'].str.contains('WORRIGEE', na=False)]
print(f"Records in WORRIGEE: {len(worrigee_records)}")

# Export filtered results
worrigee_records.to_csv('worrigee_applications.csv', index=False)
```

## Troubleshooting

### Issue: "Chrome not found" Error

**Solution**:
```bash
# The webdriver-manager should auto-download Chrome driver
# If still failing, explicitly install:
pip install --upgrade webdriver-manager
```

### Issue: Scraper Hangs on Specific Page

**Solution**:
- Increase `WAIT_TIME` in configuration:
  ```python
  WAIT_TIME = 30  # From 20 to 30 seconds
  ```
- Check internet connection
- Try running at different time

### Issue: "Element Not Found" Error

**Solution**:
- Website structure may have changed
- Update XPath selectors
- Run in non-headless mode to see what's happening

### Issue: CSV Missing Some Records

**Solution**:
- Check console for error messages
- Verify date range is correct
- Try re-running scraper
- Check browser compatibility (use latest Chrome)

### Issue: Duplicate Records in Output

**Solution**:
- Run scraper only once
- Check that scraper completed successfully
- Delete old `results.csv` before running again

### Issue: Incorrect Cleaned Values

**Solution**:
- Verify cleaning rule text matches exactly
- Check for extra whitespace or special characters
- Review extracted vs. expected values in CSV

## File Structure

```
shoalhaven-da-scraper/
‚îú‚îÄ‚îÄ scraper_final_complete_v2.py    # Main scraper script
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ LICENSE                         # MIT License
‚îú‚îÄ‚îÄ .gitignore                      # Git ignore rules
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ Scraper_Documentation.pdf   # Detailed documentation
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ results_sample.csv          # Sample output
‚îÇ   ‚îî‚îÄ‚îÄ usage_examples.py           # Example usage patterns
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_scraper.py             # Unit tests
```

## Requirements

See `requirements.txt`:

```
selenium>=4.0.0
webdriver-manager>=3.8.0
beautifulsoup4>=4.9.0
pandas>=1.3.0
```

## Performance Metrics

| Metric | Value |
|--------|-------|
| **Pages Processed** | 22 |
| **Records Collected** | 215 |
| **Processing Time** | ~35-45 minutes |
| **Success Rate** | 100% |
| **Duplicate Prevention** | 100% |
| **Data Cleaning Accuracy** | 100% |
| **CSV Export Success** | 100% |

## Browser Compatibility

| Browser | Status | Notes |
|---------|--------|-------|
| Chrome | ‚úÖ Supported | Recommended, auto-downloaded |
| Chromium | ‚úÖ Supported | Open-source variant |
| Firefox | ‚ö†Ô∏è Not Tested | Would require different driver |
| Safari | ‚ö†Ô∏è Not Tested | Would require different driver |

## API Reference

### Main Functions

#### `create_driver(headless=False)`
Creates and configures Selenium WebDriver.
- **Returns**: WebDriver instance

#### `extract_total_pages_and_items(driver)`
Extracts total pages and items from search results.
- **Returns**: Tuple (total_items, total_pages)

#### `normalize_url(raw_url)`
Converts relative URLs to absolute.
- **Args**: raw_url (str)
- **Returns**: Absolute URL (str)

#### `collect_da_and_urls(driver, total_pages)`
Collects DA numbers and normalized URLs from all pages.
- **Returns**: List of dicts with 'da' and 'url' keys

#### `extract_details_from_page(driver)`
Extracts all 12 fields from detail page with cleaning.
- **Returns**: Dict with 12 fields or None

#### `scrape_all_records(driver, da_url_list)`
Scrapes all records sequentially with deduplication.
- **Returns**: List of extracted record dicts

#### `save_records_to_csv(records, filename)`
Saves records to CSV with 12 headers.
- **Returns**: True if successful, False otherwise

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Open Pull Request

### Reporting Issues

Please report bugs with:
- Description of issue
- Steps to reproduce
- Expected vs. actual behavior
- Console error messages
- Python and OS version

## Roadmap

- [ ] Add support for other NSW councils
- [ ] Implement incremental scraping (append new records)
- [ ] Add database export (SQLite, PostgreSQL)
- [ ] Create web dashboard for results
- [ ] Add email notifications on completion
- [ ] Implement scheduling (automatic daily runs)

## License

MIT License - See [LICENSE](LICENSE) file for details.

**In summary**: You're free to use, modify, and distribute this software.

## Support

### Documentation

- üìò [Full Documentation](docs/Scraper_Documentation.pdf)
- üìñ [Function Reference](#api-reference)
- üîç [Troubleshooting](#troubleshooting)

### Getting Help

- üí¨ Open an [Issue](https://github.com/yourusername/shoalhaven-da-scraper/issues)
- üìß Email: your.email@example.com
- üí° Check [Discussions](https://github.com/yourusername/shoalhaven-da-scraper/discussions)

### Related Resources

- [Shoalhaven City Council](https://www.shoalhaven.nsw.gov.au/)
- [DA Tracking System](https://www3.shoalhaven.nsw.gov.au/masterviewUI/)
- [Selenium Documentation](https://www.selenium.dev/documentation/)
- [BeautifulSoup Guide](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

## Acknowledgments

Built with:
- [Selenium WebDriver](https://www.selenium.dev/)
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)
- [Pandas](https://pandas.pydata.org/)
- [WebDriver Manager](https://github.com/SergeyPirogov/webdriver_manager)

## Disclaimer

This scraper is provided for educational and research purposes. Users are responsible for:
- Complying with website Terms of Service
- Respecting rate limits and server resources
- Using data in accordance with applicable laws
- Not interfering with normal website operation

The author is not affiliated with Shoalhaven City Council and assumes no responsibility for:
- Data accuracy or completeness
- Website changes affecting functionality
- Misuse of collected data

## Status

![Status](https://img.shields.io/badge/status-production%20ready-brightgreen.svg)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)
![Maintenance](https://img.shields.io/badge/maintenance-active-brightgreen.svg)

**Last Updated**: November 1, 2025  
**Version**: 1.0.0  
**Author**: Web Scraping Team

---

**‚≠ê If you found this helpful, please star the repository!**

[![Star](https://img.shields.io/github/stars/yourusername/shoalhaven-da-scraper?style=social)](https://github.com/yourusername/shoalhaven-da-scraper)
[![Fork](https://img.shields.io/github/forks/yourusername/shoalhaven-da-scraper?style=social)](https://github.com/yourusername/shoalhaven-da-scraper)
